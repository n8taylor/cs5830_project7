{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86a4397f8b382ebb",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Project 7\n",
    "\n",
    "- [Report](https://docs.google.com/document/d/1d6JuigRGQrC9244Y_fzWR2EBKznfnKVnQH_Bf-OEOwI/edit?usp=sharing)\n",
    "- [Slides](https://docs.google.com/presentation/d/1qbXJJV9wEzjcOUMc-ESLRDamedZsxWrcTiVqrR-AlJ8/edit?usp=sharing)\n",
    "- [Dataset](https://www.kaggle.com/datasets/jonathanpilafas/2024-march-madness-statistical-analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664f7a90ad922e6a",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import pandas as pd\n",
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3bdcfc6",
   "metadata": {},
   "source": [
    "## Exploring/Cleaning Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd2f9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('DEV _ March Madness.csv')\n",
    "exclude_columns = [col for col in df.columns if 'Rank' in col]\n",
    "df = df.drop(columns=exclude_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a2da38",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['Team Name', 'Off.eFG %', 'Off.TO %', 'Off.OR %', 'Off.FT Rate', 'Def.eFG %', 'Def.TO %', 'Def.OR %', 'Def.FT Rate', 'Off.FT',\n",
    "       'Off.2PT FG', 'Off.3PT FG', 'Def.FT', 'Def.2PT FG', 'Def.3PT FG', 'Avg Possession Length (Offense)', \n",
    "       'Avg Possession Length (Defense)', 'Active Coaching Length', 'Post-Season Tournament']]\n",
    "\n",
    "def extract_years(years_string):\n",
    "    years = re.findall(r'\\d+', years_string)\n",
    "    if years:\n",
    "        return int(years[0])\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "df['Active Coaching Length'] = df['Active Coaching Length'].apply(extract_years)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09ad3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Average Coaching Time of Current Coach')\n",
    "plt.xlabel('Years')\n",
    "sns.barplot(data=df, x=\"Active Coaching Length\", hue='Post-Season Tournament')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3acee7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "usu = df[df['Team Name'] == 'Utah State']\n",
    "\n",
    "sns.regplot(data=df, x='Off.eFG %', y='Def.eFG %')\n",
    "plt.title('Offensive vs Defensive Field Goal Efficiency Percentage')\n",
    "plt.xlabel('Offensive field goal efficiency percentage')\n",
    "plt.ylabel('Defensive field goal efficiency percentage')\n",
    "plt.scatter(usu['Off.eFG %'], usu['Def.eFG %'], color='navy', s=150, marker='o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d650321b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[8, 6])\n",
    "sns.scatterplot(data=df, x='Off.eFG %', y='Def.eFG %', hue='Post-Season Tournament')\n",
    "plt.title('Offensive vs Defensive Field Goal Efficiency Percentage')\n",
    "plt.xlabel('Offensive field goal efficiency percentage')\n",
    "plt.ylabel('Defensive field goal efficiency percentage')\n",
    "plt.scatter(usu['Off.eFG %'], usu['Def.eFG %'], color='navy', s=150, marker='o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d900aed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.regplot(data=df, x='Off.TO %', y='Def.TO %')\n",
    "plt.title('Offensive vs Defensive Turnover Percentage')\n",
    "plt.xlabel('Offensive turnover percentage')\n",
    "plt.ylabel('Defensive turnover percentage')\n",
    "plt.scatter(usu['Off.TO %'], usu['Def.TO %'], color='navy', s=150, marker='o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc90e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.regplot(data=df, x='Off.OR %', y='Def.OR %')\n",
    "plt.title('Offensive vs Defensive Rebound Percentage')\n",
    "plt.xlabel('Offensive rebound percentage')\n",
    "plt.ylabel('Defensive rebound percentage')\n",
    "plt.scatter(usu['Off.OR %'], usu['Def.OR %'], color='navy', s=150, marker='o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02e8db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.regplot(data=df, x='Off.FT Rate', y='Def.FT Rate')\n",
    "plt.title('Offensive vs Defensive Free Throw Rate')\n",
    "plt.xlabel('Offensive free throw rate')\n",
    "plt.ylabel('Defensive free throw rate')\n",
    "plt.scatter(usu['Off.FT Rate'], usu['Def.FT Rate'], color='navy', s=150, marker='o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0175b722",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.regplot(data=df, x='Off.FT', y='Def.FT')\n",
    "plt.title('Offensive vs Defensive Free Throws')\n",
    "plt.xlabel('Offensive free throws')\n",
    "plt.ylabel('Defensive free throws')\n",
    "plt.scatter(usu['Off.FT'], usu['Def.FT'], color='navy', s=150, marker='o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7704a1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.regplot(data=df, x='Off.2PT FG', y='Def.2PT FG')\n",
    "plt.title('Offensive vs Defensive 2 Pt Field Goals')\n",
    "plt.xlabel('Offensive 2 pt field goals')\n",
    "plt.ylabel('Defensive 2 pt field goals')\n",
    "plt.scatter(usu['Off.2PT FG'], usu['Def.2PT FG'], color='navy', s=150, marker='o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b210677",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.regplot(data=df, x='Off.3PT FG', y='Def.3PT FG')\n",
    "plt.title('Offensive vs Defensive 3 Pt Field Goals')\n",
    "plt.xlabel('Offensive 3 pt field goals')\n",
    "plt.ylabel('Defensive 3 pt field goals')\n",
    "plt.scatter(usu['Off.3PT FG'], usu['Def.3PT FG'], color='navy', s=150, marker='o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6726b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.regplot(data=df, x='Avg Possession Length (Offense)', y='Avg Possession Length (Defense)')\n",
    "plt.title('Offensive vs Defensive Average Possession Length')\n",
    "plt.xlabel('Offensive average possession length')\n",
    "plt.ylabel('Defensive average possession length')\n",
    "plt.scatter(usu['Avg Possession Length (Offense)'], usu['Avg Possession Length (Defense)'], color='navy', s=150, marker='o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87c0a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'March Madness', 'Not In a Post-Season Tournament', 'NIT', 'CBI', 'CIT'\n",
    "byTourneyDF = df.groupby('Post-Season Tournament')['Team Name'].count().reset_index().sort_values('Team Name')\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(data=byTourneyDF, x='Post-Season Tournament', y='Team Name')\n",
    "plt.title('Tournament Team Counts')\n",
    "plt.xlabel('Tournament')\n",
    "plt.ylabel('Teams Qualified')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2b2044",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65bb4b73",
   "metadata": {},
   "source": [
    "### All Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cedb928",
   "metadata": {},
   "source": [
    "#### All Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59cb8a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['Off.eFG %', 'Off.TO %', 'Off.OR %', 'Off.FT Rate',\n",
    "       'Def.eFG %', 'Def.TO %', 'Def.OR %', 'Def.FT Rate', 'Off.FT',\n",
    "       'Off.2PT FG', 'Off.3PT FG', 'Def.FT', 'Def.2PT FG', 'Def.3PT FG', 'Avg Possession Length (Offense)', 'Avg Possession Length (Defense)', 'Active Coaching Length']]\n",
    "y = df['Post-Season Tournament']\n",
    "\n",
    "class_counts = df['Post-Season Tournament'].value_counts()\n",
    "display(class_counts.index)\n",
    "\n",
    "total_samples = len(df)\n",
    "\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=class_counts.index.to_numpy(), y=df['Post-Season Tournament'])\n",
    "\n",
    "weights = {}\n",
    "for index, tourney in enumerate(class_counts.index):\n",
    "    weights[tourney] = class_weights[index] \n",
    "\n",
    "lm = LogisticRegression(class_weight=weights)\n",
    "lm.fit(X, y)\n",
    "\n",
    "y_pred = lm.predict(X)\n",
    "\n",
    "p,r,f,s = precision_recall_fscore_support(y, y_pred, labels=class_counts.index)\n",
    "\n",
    "display('support = {}'.format(s))\n",
    "display('precision = {}'.format(p))\n",
    "display('recall = {}'.format(r))\n",
    "display('f-score = {}'.format(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78538733",
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = confusion_matrix(y, y_pred)\n",
    "sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('true label')\n",
    "plt.ylabel('predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30c905c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['Off.eFG %', 'Off.TO %', 'Off.OR %', 'Off.FT Rate',\n",
    "       'Def.eFG %', 'Def.TO %', 'Def.OR %', 'Def.FT Rate', 'Off.FT',\n",
    "       'Off.2PT FG', 'Off.3PT FG', 'Def.FT', 'Def.2PT FG', 'Def.3PT FG', 'Avg Possession Length (Offense)', 'Avg Possession Length (Defense)', 'Active Coaching Length']]\n",
    "y = df['Post-Season Tournament']\n",
    "\n",
    "class_counts = df['Post-Season Tournament'].value_counts()\n",
    "\n",
    "total_samples = len(df)\n",
    "\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=class_counts.index.to_numpy(), y=df['Post-Season Tournament'])\n",
    "\n",
    "weights = {}\n",
    "for index, tourney in enumerate(class_counts.index):\n",
    "    weights[tourney] = class_weights[index]\n",
    "    \n",
    "scores = {'p': [], 'r': [], 'f': []}\n",
    "for _ in range(30):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)#, random_state=42)\n",
    "\n",
    "    lm = LogisticRegression(class_weight=weights)\n",
    "    lm.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = lm.predict(X_test)\n",
    "\n",
    "    p,r,f,s = precision_recall_fscore_support(y_test, y_pred, labels=class_counts.index)\n",
    "    scores['p'].append(p)\n",
    "    scores['r'].append(r)\n",
    "    scores['f'].append(f)\n",
    "    \n",
    "avgP = 0\n",
    "for pre in scores['p']:\n",
    "    avgP += pre\n",
    "avgP /= len(scores['p'])\n",
    "\n",
    "avgR = 0\n",
    "for re in scores['r']:\n",
    "    avgR += re\n",
    "avgR /= len(scores['r'])\n",
    "\n",
    "avgF = 0\n",
    "for f in scores['f']:\n",
    "    avgF += f\n",
    "avgF /= len(scores['f'])\n",
    "\n",
    "display('precision = {}'.format(avgP))\n",
    "display('recall = {}'.format(avgR))\n",
    "display('f-score = {}'.format(avgF))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf3aba45",
   "metadata": {},
   "source": [
    "#### Offensive Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301f12f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['Off.eFG %', 'Off.TO %', 'Off.OR %', 'Off.FT Rate',\n",
    "        'Off.FT',\n",
    "       'Off.2PT FG', 'Off.3PT FG',  'Avg Possession Length (Offense)',  'Active Coaching Length']]\n",
    "y = df['Post-Season Tournament']\n",
    "\n",
    "class_counts = df['Post-Season Tournament'].value_counts()\n",
    "\n",
    "total_samples = len(df)\n",
    "\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=class_counts.index.to_numpy(), y=df['Post-Season Tournament'])\n",
    "\n",
    "weights = {}\n",
    "for index, tourney in enumerate(class_counts.index):\n",
    "    weights[tourney] = class_weights[index]\n",
    "    \n",
    "scores = {'p': [], 'r': [], 'f': []}\n",
    "for _ in range(30):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)#, random_state=42)\n",
    "\n",
    "    lm = LogisticRegression(class_weight=weights)\n",
    "    lm.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = lm.predict(X_test)\n",
    "\n",
    "    p,r,f,s = precision_recall_fscore_support(y_test, y_pred, labels=class_counts.index)\n",
    "    scores['p'].append(p)\n",
    "    scores['r'].append(r)\n",
    "    scores['f'].append(f)\n",
    "\n",
    "avgP = 0\n",
    "for pre in scores['p']:\n",
    "    avgP += pre\n",
    "avgP /= len(scores['p'])\n",
    "\n",
    "avgR = 0\n",
    "for re in scores['r']:\n",
    "    avgR += re\n",
    "avgR /= len(scores['r'])\n",
    "\n",
    "avgF = 0\n",
    "for f in scores['f']:\n",
    "    avgF += f\n",
    "avgF /= len(scores['f'])\n",
    "\n",
    "display('precision = {}'.format(avgP))\n",
    "display('recall = {}'.format(avgR))\n",
    "display('f-score = {}'.format(avgF))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb18e069",
   "metadata": {},
   "source": [
    "#### Defensive Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c842943f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[[       'Def.eFG %', 'Def.TO %', 'Def.OR %', 'Def.FT Rate', \n",
    "        'Def.FT', 'Def.2PT FG', 'Def.3PT FG',  'Avg Possession Length (Defense)', 'Active Coaching Length']]\n",
    "y = df['Post-Season Tournament']\n",
    "\n",
    "class_counts = df['Post-Season Tournament'].value_counts()\n",
    "\n",
    "total_samples = len(df)\n",
    "\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=class_counts.index.to_numpy(), y=df['Post-Season Tournament'])\n",
    "\n",
    "weights = {}\n",
    "for index, tourney in enumerate(class_counts.index):\n",
    "    weights[tourney] = class_weights[index]\n",
    "    \n",
    "scores = {'p': [], 'r': [], 'f': []}\n",
    "for _ in range(30):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)#, random_state=42)\n",
    "\n",
    "    lm = LogisticRegression(class_weight=weights)\n",
    "    lm.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = lm.predict(X_test)\n",
    "\n",
    "    p,r,f,s = precision_recall_fscore_support(y_test, y_pred, labels=class_counts.index)\n",
    "    scores['p'].append(p)\n",
    "    scores['r'].append(r)\n",
    "    scores['f'].append(f)\n",
    "\n",
    "avgP = 0\n",
    "for pre in scores['p']:\n",
    "    avgP += pre\n",
    "avgP /= len(scores['p'])\n",
    "\n",
    "avgR = 0\n",
    "for re in scores['r']:\n",
    "    avgR += re\n",
    "avgR /= len(scores['r'])\n",
    "\n",
    "avgF = 0\n",
    "for f in scores['f']:\n",
    "    avgF += f\n",
    "avgF /= len(scores['f'])\n",
    "\n",
    "display('precision = {}'.format(avgP))\n",
    "display('recall = {}'.format(avgR))\n",
    "display('f-score = {}'.format(avgF))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e2902f",
   "metadata": {},
   "source": [
    "#### Coaching Attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a412dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['Active Coaching Length']]\n",
    "y = df['Post-Season Tournament']\n",
    "\n",
    "class_counts = df['Post-Season Tournament'].value_counts()\n",
    "\n",
    "total_samples = len(df)\n",
    "\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=class_counts.index.to_numpy(), y=df['Post-Season Tournament'])\n",
    "\n",
    "weights = {}\n",
    "for index, tourney in enumerate(class_counts.index):\n",
    "    weights[tourney] = class_weights[index]\n",
    "    \n",
    "scores = {'p': [], 'r': [], 'f': []}\n",
    "for _ in range(30):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)#, random_state=42)\n",
    "\n",
    "    lm = LogisticRegression(class_weight=weights)\n",
    "    lm.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = lm.predict(X_test)\n",
    "\n",
    "    p,r,f,s = precision_recall_fscore_support(y_test, y_pred, labels=class_counts.index)\n",
    "    scores['p'].append(p)\n",
    "    scores['r'].append(r)\n",
    "    scores['f'].append(f)\n",
    "\n",
    "avgP = 0\n",
    "for pre in scores['p']:\n",
    "    avgP += pre\n",
    "avgP /= len(scores['p'])\n",
    "\n",
    "avgR = 0\n",
    "for re in scores['r']:\n",
    "    avgR += re\n",
    "avgR /= len(scores['r'])\n",
    "\n",
    "avgF = 0\n",
    "for f in scores['f']:\n",
    "    avgF += f\n",
    "avgF /= len(scores['f'])\n",
    "\n",
    "display('precision = {}'.format(avgP))\n",
    "display('recall = {}'.format(avgR))\n",
    "display('f-score = {}'.format(avgF))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59cc2a82",
   "metadata": {},
   "source": [
    "### Grouped Into Two Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c099d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(value):\n",
    "    if value != 'March Madness':\n",
    "        return 0\n",
    "    return 1\n",
    "\n",
    "binaryDF = df.copy()\n",
    "binaryDF['Post-Season Tournament'] = [transform(value) for value in binaryDF['Post-Season Tournament']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cacf8a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(data=binaryDF, x=\"Active Coaching Length\", hue='Post-Season Tournament')\n",
    "plt.title('Average Coaching Time of Current Coach')\n",
    "plt.xlabel('Years')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13783f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[12, 6])\n",
    "sns.scatterplot(data=binaryDF, x='Off.eFG %', y='Def.eFG %', hue='Post-Season Tournament')\n",
    "plt.title('Offensive vs Defensive Field Goal Efficiency Percentage')\n",
    "plt.xlabel('Offensive field goal efficiency percentage')\n",
    "plt.ylabel('Defensive field goal efficiency percentage')\n",
    "plt.scatter(usu['Off.eFG %'], usu['Def.eFG %'], color='navy', s=150, marker='o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0b4913",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[12, 6])\n",
    "sns.scatterplot(data=binaryDF, x='Off.2PT FG', y='Def.2PT FG', hue='Post-Season Tournament')\n",
    "plt.title('Offensive vs Defensive 2 Pt Field Goals')\n",
    "plt.xlabel('Offensive 2 pt field goals')\n",
    "plt.ylabel('Defensive 2 pt field goals')\n",
    "plt.scatter(usu['Off.2PT FG'], usu['Def.2PT FG'], color='navy', s=150, marker='o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab0594f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[12, 6])\n",
    "sns.scatterplot(data=binaryDF, x='Off.3PT FG', y='Def.3PT FG', hue='Post-Season Tournament')\n",
    "plt.title('Offensive vs Defensive 3 Pt Field Goals')\n",
    "plt.xlabel('Offensive 3 pt fields')\n",
    "plt.ylabel('Defensive 3 pt fields')\n",
    "plt.scatter(usu['Off.3PT FG'], usu['Def.3PT FG'], color='navy', s=150, marker='o')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176c8f46",
   "metadata": {},
   "source": [
    "#### All Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6979de8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = binaryDF[['Off.eFG %', 'Off.TO %', 'Off.OR %', 'Off.FT Rate',\n",
    "       'Def.eFG %', 'Def.TO %', 'Def.OR %', 'Def.FT Rate', 'Off.FT',\n",
    "       'Off.2PT FG', 'Off.3PT FG', 'Def.FT', 'Def.2PT FG', 'Def.3PT FG', 'Avg Possession Length (Offense)', 'Avg Possession Length (Defense)', 'Active Coaching Length']]\n",
    "y = binaryDF['Post-Season Tournament']\n",
    "\n",
    "class_counts = binaryDF['Post-Season Tournament'].value_counts()\n",
    "\n",
    "total_samples = len(binaryDF)\n",
    "\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=class_counts.index.to_numpy(), y=binaryDF['Post-Season Tournament'])\n",
    "\n",
    "weights = {}\n",
    "for index, tourney in enumerate(class_counts.index):\n",
    "    weights[tourney] = class_weights[index]\n",
    "\n",
    "lm = LogisticRegression(class_weight=weights)\n",
    "lm.fit(X, y)\n",
    "\n",
    "y_pred = lm.predict(X)\n",
    "\n",
    "p,r,f,s = precision_recall_fscore_support(y, y_pred, labels=class_counts.index)\n",
    "display(s)\n",
    "display('precision = {}'.format(p))\n",
    "display('recall = {}'.format(r))\n",
    "display('f-score = {}'.format(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb89483d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = confusion_matrix(y, y_pred)\n",
    "sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('true label')\n",
    "plt.ylabel('predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae974e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = binaryDF[['Off.eFG %', 'Off.TO %', 'Off.OR %', 'Off.FT Rate',\n",
    "       'Def.eFG %', 'Def.TO %', 'Def.OR %', 'Def.FT Rate', 'Off.FT',\n",
    "       'Off.2PT FG', 'Off.3PT FG', 'Def.FT', 'Def.2PT FG', 'Def.3PT FG', 'Avg Possession Length (Offense)', 'Avg Possession Length (Defense)', 'Active Coaching Length']]\n",
    "y = binaryDF['Post-Season Tournament']\n",
    "\n",
    "class_counts = binaryDF['Post-Season Tournament'].value_counts()\n",
    "\n",
    "total_samples = len(binaryDF)\n",
    "\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=class_counts.index.to_numpy(), y=binaryDF['Post-Season Tournament'])\n",
    "\n",
    "weights = {}\n",
    "for index, tourney in enumerate(class_counts.index):\n",
    "    weights[tourney] = class_weights[index]\n",
    "    \n",
    "scores = {'p': [], 'r': [], 'f': []}\n",
    "for _ in range(30):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)#, random_state=42)\n",
    "\n",
    "    lm = LogisticRegression(class_weight=weights)\n",
    "    lm.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = lm.predict(X_test)\n",
    "\n",
    "    p,r,f,s = precision_recall_fscore_support(y_test, y_pred, labels=class_counts.index)\n",
    "    scores['p'].append(p)\n",
    "    scores['r'].append(r)\n",
    "    scores['f'].append(f)\n",
    "\n",
    "avgP = 0\n",
    "for pre in scores['p']:\n",
    "    avgP += pre\n",
    "avgP /= len(scores['p'])\n",
    "\n",
    "avgR = 0\n",
    "for re in scores['r']:\n",
    "    avgR += re\n",
    "avgR /= len(scores['r'])\n",
    "\n",
    "avgF = 0\n",
    "for f in scores['f']:\n",
    "    avgF += f\n",
    "avgF /= len(scores['f'])\n",
    "\n",
    "display('precision = {}'.format(avgP))\n",
    "display('recall = {}'.format(avgR))\n",
    "display('f-score = {}'.format(avgF))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2634ea4",
   "metadata": {},
   "source": [
    "#### Offensive Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892f9c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = binaryDF[['Off.eFG %', 'Off.TO %', 'Off.OR %', 'Off.FT Rate',\n",
    "        'Off.FT',\n",
    "       'Off.2PT FG', 'Off.3PT FG',  'Avg Possession Length (Offense)',  'Active Coaching Length']]\n",
    "y = binaryDF['Post-Season Tournament']\n",
    "\n",
    "class_counts = binaryDF['Post-Season Tournament'].value_counts()\n",
    "\n",
    "total_samples = len(binaryDF)\n",
    "\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=class_counts.index.to_numpy(), y=binaryDF['Post-Season Tournament'])\n",
    "\n",
    "weights = {}\n",
    "for index, tourney in enumerate(class_counts.index):\n",
    "    weights[tourney] = class_weights[index]\n",
    "    \n",
    "scores = {'p': [], 'r': [], 'f': []}\n",
    "for _ in range(30):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)#, random_state=42)\n",
    "\n",
    "    lm = LogisticRegression(class_weight=weights)\n",
    "    lm.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = lm.predict(X_test)\n",
    "\n",
    "    p,r,f,s = precision_recall_fscore_support(y_test, y_pred, labels=class_counts.index)\n",
    "    scores['p'].append(p)\n",
    "    scores['r'].append(r)\n",
    "    scores['f'].append(f)\n",
    "\n",
    "avgP = 0\n",
    "for pre in scores['p']:\n",
    "    avgP += pre\n",
    "avgP /= len(scores['p'])\n",
    "\n",
    "avgR = 0\n",
    "for re in scores['r']:\n",
    "    avgR += re\n",
    "avgR /= len(scores['r'])\n",
    "\n",
    "avgF = 0\n",
    "for f in scores['f']:\n",
    "    avgF += f\n",
    "avgF /= len(scores['f'])\n",
    "\n",
    "display('precision = {}'.format(avgP))\n",
    "display('recall = {}'.format(avgR))\n",
    "display('f-score = {}'.format(avgF))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b1c6a8",
   "metadata": {},
   "source": [
    "#### Defensive Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d02a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = binaryDF[[\n",
    "       'Def.eFG %', 'Def.TO %', 'Def.OR %', 'Def.FT Rate', \n",
    "     'Def.FT', 'Def.2PT FG', 'Def.3PT FG', 'Avg Possession Length (Defense)', 'Active Coaching Length']]\n",
    "y = binaryDF['Post-Season Tournament']\n",
    "\n",
    "class_counts = binaryDF['Post-Season Tournament'].value_counts()\n",
    "\n",
    "total_samples = len(binaryDF)\n",
    "\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=class_counts.index.to_numpy(), y=binaryDF['Post-Season Tournament'])\n",
    "\n",
    "weights = {}\n",
    "for index, tourney in enumerate(class_counts.index):\n",
    "    weights[tourney] = class_weights[index]\n",
    "    \n",
    "scores = {'p': [], 'r': [], 'f': []}\n",
    "for _ in range(30):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)#, random_state=42)\n",
    "\n",
    "    lm = LogisticRegression(class_weight=weights)\n",
    "    lm.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = lm.predict(X_test)\n",
    "\n",
    "    p,r,f,s = precision_recall_fscore_support(y_test, y_pred, labels=class_counts.index)\n",
    "    scores['p'].append(p)\n",
    "    scores['r'].append(r)\n",
    "    scores['f'].append(f)\n",
    "\n",
    "avgP = 0\n",
    "for pre in scores['p']:\n",
    "    avgP += pre\n",
    "avgP /= len(scores['p'])\n",
    "\n",
    "avgR = 0\n",
    "for re in scores['r']:\n",
    "    avgR += re\n",
    "avgR /= len(scores['r'])\n",
    "\n",
    "avgF = 0\n",
    "for f in scores['f']:\n",
    "    avgF += f\n",
    "avgF /= len(scores['f'])\n",
    "\n",
    "display('precision = {}'.format(avgP))\n",
    "display('recall = {}'.format(avgR))\n",
    "display('f-score = {}'.format(avgF))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efafb7b7",
   "metadata": {},
   "source": [
    "#### Coach Attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7741adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = binaryDF[['Active Coaching Length']]\n",
    "y = binaryDF['Post-Season Tournament']\n",
    "\n",
    "class_counts = binaryDF['Post-Season Tournament'].value_counts()\n",
    "\n",
    "total_samples = len(binaryDF)\n",
    "\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=class_counts.index.to_numpy(), y=binaryDF['Post-Season Tournament'])\n",
    "\n",
    "weights = {}\n",
    "for index, tourney in enumerate(class_counts.index):\n",
    "    weights[tourney] = class_weights[index]\n",
    "    \n",
    "scores = {'p': [], 'r': [], 'f': []}\n",
    "for _ in range(30):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)#, random_state=42)\n",
    "\n",
    "    lm = LogisticRegression(class_weight=weights)\n",
    "    lm.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = lm.predict(X_test)\n",
    "\n",
    "    p,r,f,s = precision_recall_fscore_support(y_test, y_pred, labels=class_counts.index)\n",
    "    scores['p'].append(p)\n",
    "    scores['r'].append(r)\n",
    "    scores['f'].append(f)\n",
    "\n",
    "avgP = 0\n",
    "for pre in scores['p']:\n",
    "    avgP += pre\n",
    "avgP /= len(scores['p'])\n",
    "\n",
    "avgR = 0\n",
    "for re in scores['r']:\n",
    "    avgR += re\n",
    "avgR /= len(scores['r'])\n",
    "\n",
    "avgF = 0\n",
    "for f in scores['f']:\n",
    "    avgF += f\n",
    "avgF /= len(scores['f'])\n",
    "\n",
    "display('precision = {}'.format(avgP))\n",
    "display('recall = {}'.format(avgR))\n",
    "display('f-score = {}'.format(avgF))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffdc0199",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ea6562",
   "metadata": {},
   "source": [
    "### All Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece2e7c4",
   "metadata": {},
   "source": [
    "#### All Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f63b029",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['Off.eFG %', 'Off.TO %', 'Off.OR %', 'Off.FT Rate',\n",
    "       'Def.eFG %', 'Def.TO %', 'Def.OR %', 'Def.FT Rate', 'Off.FT',\n",
    "       'Off.2PT FG', 'Off.3PT FG', 'Def.FT', 'Def.2PT FG', 'Def.3PT FG', 'Avg Possession Length (Offense)', 'Avg Possession Length (Defense)', 'Active Coaching Length']]\n",
    "y = df['Post-Season Tournament']\n",
    "\n",
    "class_counts = df['Post-Season Tournament'].value_counts()\n",
    "\n",
    "total_samples = len(df)\n",
    "\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=class_counts.index.to_numpy(), y=df['Post-Season Tournament'])\n",
    "\n",
    "weights = {}\n",
    "for index, tourney in enumerate(class_counts.index):\n",
    "    weights[tourney] = class_weights[index]\n",
    "\n",
    "scores = {'p': [], 'r': [], 'f': []}\n",
    "for _ in range(30):\n",
    "       X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)#, random_state=42)\n",
    "\n",
    "    #    clf = svm.SVC(kernel='linear', class_weight=weights)\n",
    "       clf = svm.SVC(kernel='poly', degree=5, class_weight=weights)#, class_weight={0:1, 1:0.2}) # try degree=2,4\n",
    "    #    clf = svm.SVC(kernel='rbf', gamma=90, class_weight=weights) # try gamma=.1,1\n",
    "       clf.fit(X_train, y_train)\n",
    "\n",
    "       y_pred = clf.predict(X_test)\n",
    "       p,r,f,s = precision_recall_fscore_support(y_test, y_pred, labels=class_counts.index)\n",
    "       scores['p'].append(p)\n",
    "       scores['r'].append(r)\n",
    "       scores['f'].append(f)\n",
    "\n",
    "avgP = 0\n",
    "for pre in scores['p']:\n",
    "    avgP += pre\n",
    "avgP /= len(scores['p'])\n",
    "\n",
    "avgR = 0\n",
    "for re in scores['r']:\n",
    "    avgR += re\n",
    "avgR /= len(scores['r'])\n",
    "\n",
    "avgF = 0\n",
    "for f in scores['f']:\n",
    "    avgF += f\n",
    "avgF /= len(scores['f'])\n",
    "\n",
    "display('precision = {}'.format(avgP))\n",
    "display('recall = {}'.format(avgR))\n",
    "display('f-score = {}'.format(avgF))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370b883e",
   "metadata": {},
   "source": [
    "#### Offensive Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf88d8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['Off.eFG %', 'Off.TO %', 'Off.OR %', 'Off.FT Rate',\n",
    "        'Off.FT',\n",
    "       'Off.2PT FG', 'Off.3PT FG',  'Avg Possession Length (Offense)', 'Active Coaching Length']]\n",
    "y = df['Post-Season Tournament']\n",
    "\n",
    "class_counts = df['Post-Season Tournament'].value_counts()\n",
    "\n",
    "total_samples = len(df)\n",
    "\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=class_counts.index.to_numpy(), y=df['Post-Season Tournament'])\n",
    "\n",
    "weights = {}\n",
    "for index, tourney in enumerate(class_counts.index):\n",
    "    weights[tourney] = class_weights[index]\n",
    "\n",
    "scores = {'p': [], 'r': [], 'f': []}\n",
    "for _ in range(30):\n",
    "       X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)#, random_state=42)\n",
    "\n",
    "    #    clf = svm.SVC(kernel='linear', class_weight=weights)\n",
    "       clf = svm.SVC(kernel='poly', degree=5, class_weight=weights)#, class_weight={0:1, 1:0.2}) # try degree=2,4\n",
    "    #    clf = svm.SVC(kernel='rbf', gamma=90, class_weight=weights) # try gamma=.1,1\n",
    "       clf.fit(X_train, y_train)\n",
    "\n",
    "       y_pred = clf.predict(X_test)\n",
    "       p,r,f,s = precision_recall_fscore_support(y_test, y_pred, labels=class_counts.index)\n",
    "       scores['p'].append(p)\n",
    "       scores['r'].append(r)\n",
    "       scores['f'].append(f)\n",
    "\n",
    "avgP = 0\n",
    "for pre in scores['p']:\n",
    "    avgP += pre\n",
    "avgP /= len(scores['p'])\n",
    "\n",
    "avgR = 0\n",
    "for re in scores['r']:\n",
    "    avgR += re\n",
    "avgR /= len(scores['r'])\n",
    "\n",
    "avgF = 0\n",
    "for f in scores['f']:\n",
    "    avgF += f\n",
    "avgF /= len(scores['f'])\n",
    "\n",
    "display('precision = {}'.format(avgP))\n",
    "display('recall = {}'.format(avgR))\n",
    "display('f-score = {}'.format(avgF))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe7c7d4",
   "metadata": {},
   "source": [
    "#### Defensive Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c09243a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[[\n",
    "       'Def.eFG %', 'Def.TO %', 'Def.OR %', 'Def.FT Rate', \n",
    "       'Def.FT', 'Def.2PT FG', 'Def.3PT FG', 'Avg Possession Length (Defense)', 'Active Coaching Length']]\n",
    "y = df['Post-Season Tournament']\n",
    "\n",
    "class_counts = df['Post-Season Tournament'].value_counts()\n",
    "\n",
    "total_samples = len(df)\n",
    "\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=class_counts.index.to_numpy(), y=df['Post-Season Tournament'])\n",
    "\n",
    "weights = {}\n",
    "for index, tourney in enumerate(class_counts.index):\n",
    "    weights[tourney] = class_weights[index]\n",
    "\n",
    "scores = {'p': [], 'r': [], 'f': []}\n",
    "for _ in range(30):\n",
    "       X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)#, random_state=42)\n",
    "\n",
    "    #    clf = svm.SVC(kernel='linear', class_weight=weights)\n",
    "       clf = svm.SVC(kernel='poly', degree=5, class_weight=weights)#, class_weight={0:1, 1:0.2}) # try degree=2,4\n",
    "    #    clf = svm.SVC(kernel='rbf', gamma=90, class_weight=weights) # try gamma=.1,1\n",
    "       clf.fit(X_train, y_train)\n",
    "\n",
    "       y_pred = clf.predict(X_test)\n",
    "       p,r,f,s = precision_recall_fscore_support(y_test, y_pred, labels=class_counts.index)\n",
    "       scores['p'].append(p)\n",
    "       scores['r'].append(r)\n",
    "       scores['f'].append(f)\n",
    "\n",
    "avgP = 0\n",
    "for pre in scores['p']:\n",
    "    avgP += pre\n",
    "avgP /= len(scores['p'])\n",
    "\n",
    "avgR = 0\n",
    "for re in scores['r']:\n",
    "    avgR += re\n",
    "avgR /= len(scores['r'])\n",
    "\n",
    "avgF = 0\n",
    "for f in scores['f']:\n",
    "    avgF += f\n",
    "avgF /= len(scores['f'])\n",
    "\n",
    "display('precision = {}'.format(avgP))\n",
    "display('recall = {}'.format(avgR))\n",
    "display('f-score = {}'.format(avgF))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff8eb53",
   "metadata": {},
   "source": [
    "#### Coach Attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a811f9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['Active Coaching Length']]\n",
    "y = df['Post-Season Tournament']\n",
    "\n",
    "class_counts = df['Post-Season Tournament'].value_counts()\n",
    "\n",
    "total_samples = len(df)\n",
    "\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=class_counts.index.to_numpy(), y=df['Post-Season Tournament'])\n",
    "\n",
    "weights = {}\n",
    "for index, tourney in enumerate(class_counts.index):\n",
    "    weights[tourney] = class_weights[index]\n",
    "\n",
    "scores = {'p': [], 'r': [], 'f': []}\n",
    "for _ in range(30):\n",
    "       X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)#, random_state=42)\n",
    "\n",
    "       clf = svm.SVC(kernel='linear', class_weight=weights)\n",
    "    #    clf = svm.SVC(kernel='poly', degree=5, class_weight=weights)#, class_weight={0:1, 1:0.2}) # try degree=2,4\n",
    "    #    clf = svm.SVC(kernel='rbf', gamma=90, class_weight=weights) # try gamma=.1,1\n",
    "       clf.fit(X_train, y_train)\n",
    "\n",
    "       y_pred = clf.predict(X_test)\n",
    "       p,r,f,s = precision_recall_fscore_support(y_test, y_pred, labels=class_counts.index)\n",
    "       scores['p'].append(p)\n",
    "       scores['r'].append(r)\n",
    "       scores['f'].append(f)\n",
    "\n",
    "avgP = 0\n",
    "for pre in scores['p']:\n",
    "    avgP += pre\n",
    "avgP /= len(scores['p'])\n",
    "\n",
    "avgR = 0\n",
    "for re in scores['r']:\n",
    "    avgR += re\n",
    "avgR /= len(scores['r'])\n",
    "\n",
    "avgF = 0\n",
    "for f in scores['f']:\n",
    "    avgF += f\n",
    "avgF /= len(scores['f'])\n",
    "\n",
    "display('precision = {}'.format(avgP))\n",
    "display('recall = {}'.format(avgR))\n",
    "display('f-score = {}'.format(avgF))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44cb41df",
   "metadata": {},
   "source": [
    "### Classes Grouped Into Two Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524e9408",
   "metadata": {},
   "source": [
    "#### All Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc67581d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = binaryDF[['Off.eFG %', 'Off.TO %', 'Off.OR %', 'Off.FT Rate',\n",
    "       'Def.eFG %', 'Def.TO %', 'Def.OR %', 'Def.FT Rate', 'Off.FT',\n",
    "       'Off.2PT FG', 'Off.3PT FG', 'Def.FT', 'Def.2PT FG', 'Def.3PT FG', 'Avg Possession Length (Offense)', 'Avg Possession Length (Defense)', 'Active Coaching Length']]\n",
    "y = binaryDF['Post-Season Tournament']\n",
    "\n",
    "class_counts = binaryDF['Post-Season Tournament'].value_counts()\n",
    "\n",
    "total_samples = len(binaryDF)\n",
    "\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=class_counts.index.to_numpy(), y=binaryDF['Post-Season Tournament'])\n",
    "\n",
    "weights = {}\n",
    "for index, tourney in enumerate(class_counts.index):\n",
    "    weights[tourney] = class_weights[index]\n",
    "\n",
    "scores = {'p': [], 'r': [], 'f': []}\n",
    "for _ in range(30):\n",
    "       X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)#, random_state=42)\n",
    "\n",
    "    #    clf = svm.SVC(kernel='linear', class_weight=weights)\n",
    "       clf = svm.SVC(kernel='poly', degree=2, class_weight=weights)#, class_weight={0:1, 1:0.2}) # try degree=2,4\n",
    "    #    clf = svm.SVC(kernel='rbf', gamma=90, class_weight=weights) # try gamma=.1,1\n",
    "       clf.fit(X_train, y_train)\n",
    "\n",
    "       y_pred = clf.predict(X_test)\n",
    "       p,r,f,s = precision_recall_fscore_support(y_test, y_pred, labels=class_counts.index)\n",
    "       scores['p'].append(p)\n",
    "       scores['r'].append(r)\n",
    "       scores['f'].append(f)\n",
    "\n",
    "avgP = 0\n",
    "for pre in scores['p']:\n",
    "    avgP += pre\n",
    "avgP /= len(scores['p'])\n",
    "\n",
    "avgR = 0\n",
    "for re in scores['r']:\n",
    "    avgR += re\n",
    "avgR /= len(scores['r'])\n",
    "\n",
    "avgF = 0\n",
    "for f in scores['f']:\n",
    "    avgF += f\n",
    "avgF /= len(scores['f'])\n",
    "\n",
    "display('precision = {}'.format(avgP))\n",
    "display('recall = {}'.format(avgR))\n",
    "display('f-score = {}'.format(avgF))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dbb0ec5",
   "metadata": {},
   "source": [
    "#### Offensive Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654562aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = binaryDF[['Off.eFG %', 'Off.TO %', 'Off.OR %', 'Off.FT Rate',\n",
    "        'Off.FT',\n",
    "       'Off.2PT FG', 'Off.3PT FG',  'Avg Possession Length (Offense)',  'Active Coaching Length']]\n",
    "y = binaryDF['Post-Season Tournament']\n",
    "\n",
    "class_counts = binaryDF['Post-Season Tournament'].value_counts()\n",
    "\n",
    "total_samples = len(binaryDF)\n",
    "\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=class_counts.index.to_numpy(), y=binaryDF['Post-Season Tournament'])\n",
    "\n",
    "weights = {}\n",
    "for index, tourney in enumerate(class_counts.index):\n",
    "    weights[tourney] = class_weights[index]\n",
    "\n",
    "scores = {'p': [], 'r': [], 'f': []}\n",
    "for _ in range(30):\n",
    "       X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)#, random_state=42)\n",
    "\n",
    "    #    clf = svm.SVC(kernel='linear', class_weight=weights)\n",
    "       clf = svm.SVC(kernel='poly', degree=2, class_weight=weights)#, class_weight={0:1, 1:0.2}) # try degree=2,4\n",
    "    #    clf = svm.SVC(kernel='rbf', gamma=90, class_weight=weights) # try gamma=.1,1\n",
    "       clf.fit(X_train, y_train)\n",
    "\n",
    "       y_pred = clf.predict(X_test)\n",
    "       p,r,f,s = precision_recall_fscore_support(y_test, y_pred, labels=class_counts.index)\n",
    "       scores['p'].append(p)\n",
    "       scores['r'].append(r)\n",
    "       scores['f'].append(f)\n",
    "\n",
    "avgP = 0\n",
    "for pre in scores['p']:\n",
    "    avgP += pre\n",
    "avgP /= len(scores['p'])\n",
    "\n",
    "avgR = 0\n",
    "for re in scores['r']:\n",
    "    avgR += re\n",
    "avgR /= len(scores['r'])\n",
    "\n",
    "avgF = 0\n",
    "for f in scores['f']:\n",
    "    avgF += f\n",
    "avgF /= len(scores['f'])\n",
    "\n",
    "display('precision = {}'.format(avgP))\n",
    "display('recall = {}'.format(avgR))\n",
    "display('f-score = {}'.format(avgF))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c13c6f8",
   "metadata": {},
   "source": [
    "#### Defensive Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98ca0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = binaryDF[[\n",
    "       'Def.eFG %', 'Def.TO %', 'Def.OR %', 'Def.FT Rate', \n",
    "        'Def.FT', 'Def.2PT FG', 'Def.3PT FG',  'Avg Possession Length (Defense)', 'Active Coaching Length']]\n",
    "y = binaryDF['Post-Season Tournament']\n",
    "\n",
    "class_counts = binaryDF['Post-Season Tournament'].value_counts()\n",
    "\n",
    "total_samples = len(binaryDF)\n",
    "\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=class_counts.index.to_numpy(), y=binaryDF['Post-Season Tournament'])\n",
    "\n",
    "weights = {}\n",
    "for index, tourney in enumerate(class_counts.index):\n",
    "    weights[tourney] = class_weights[index]\n",
    "\n",
    "scores = {'p': [], 'r': [], 'f': []}\n",
    "for _ in range(30):\n",
    "       X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)#, random_state=42)\n",
    "\n",
    "    #    clf = svm.SVC(kernel='linear', class_weight=weights)\n",
    "       clf = svm.SVC(kernel='poly', degree=2, class_weight=weights)#, class_weight={0:1, 1:0.2}) # try degree=2,4\n",
    "    #    clf = svm.SVC(kernel='rbf', gamma=90, class_weight=weights) # try gamma=.1,1\n",
    "       clf.fit(X_train, y_train)\n",
    "\n",
    "       y_pred = clf.predict(X_test)\n",
    "       p,r,f,s = precision_recall_fscore_support(y_test, y_pred, labels=class_counts.index)\n",
    "       scores['p'].append(p)\n",
    "       scores['r'].append(r)\n",
    "       scores['f'].append(f)\n",
    "\n",
    "avgP = 0\n",
    "for pre in scores['p']:\n",
    "    avgP += pre\n",
    "avgP /= len(scores['p'])\n",
    "\n",
    "avgR = 0\n",
    "for re in scores['r']:\n",
    "    avgR += re\n",
    "avgR /= len(scores['r'])\n",
    "\n",
    "avgF = 0\n",
    "for f in scores['f']:\n",
    "    avgF += f\n",
    "avgF /= len(scores['f'])\n",
    "\n",
    "display('precision = {}'.format(avgP))\n",
    "display('recall = {}'.format(avgR))\n",
    "display('f-score = {}'.format(avgF))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7397119e",
   "metadata": {},
   "source": [
    "#### Coach Attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8090b1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = binaryDF[['Active Coaching Length']]\n",
    "y = binaryDF['Post-Season Tournament']\n",
    "\n",
    "class_counts = binaryDF['Post-Season Tournament'].value_counts()\n",
    "\n",
    "total_samples = len(binaryDF)\n",
    "\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=class_counts.index.to_numpy(), y=binaryDF['Post-Season Tournament'])\n",
    "\n",
    "weights = {}\n",
    "for index, tourney in enumerate(class_counts.index):\n",
    "    weights[tourney] = class_weights[index]\n",
    "\n",
    "scores = {'p': [], 'r': [], 'f': []}\n",
    "for _ in range(30):\n",
    "       X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)#, random_state=42)\n",
    "\n",
    "    #    clf = svm.SVC(kernel='linear', class_weight=weights)\n",
    "       clf = svm.SVC(kernel='poly', degree=2, class_weight=weights)#, class_weight={0:1, 1:0.2}) # try degree=2,4\n",
    "    #    clf = svm.SVC(kernel='rbf', gamma=90, class_weight=weights) # try gamma=.1,1\n",
    "       clf.fit(X_train, y_train)\n",
    "\n",
    "       y_pred = clf.predict(X_test)\n",
    "       p,r,f,s = precision_recall_fscore_support(y_test, y_pred, labels=class_counts.index)\n",
    "       scores['p'].append(p)\n",
    "       scores['r'].append(r)\n",
    "       scores['f'].append(f)\n",
    "\n",
    "avgP = 0\n",
    "for pre in scores['p']:\n",
    "    avgP += pre\n",
    "avgP /= len(scores['p'])\n",
    "\n",
    "avgR = 0\n",
    "for re in scores['r']:\n",
    "    avgR += re\n",
    "avgR /= len(scores['r'])\n",
    "\n",
    "avgF = 0\n",
    "for f in scores['f']:\n",
    "    avgF += f\n",
    "avgF /= len(scores['f'])\n",
    "\n",
    "display('precision = {}'.format(avgP))\n",
    "display('recall = {}'.format(avgR))\n",
    "display('f-score = {}'.format(avgF))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
